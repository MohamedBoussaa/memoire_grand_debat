{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling for 20-Newsgroups\n",
    "\n",
    "There are several approaches to topic modeling. The most popular options are Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF). In this notebook we will use the 20-newsgroups dataset to compare these methods with probabilistic Latent Semantic Analysis (pLSA) and ensemble topic modeling (EnsTop) from the enstop library. This is not meant to be a particularly complete or comprehensive comparison, but rather a means to show how the enstop library works, and provide a quick comparison to other popular approaches.\n",
    "\n",
    "First we'll need the requisite libraries. Fortunately sklearn has a function to get the 20-newsgroups dataset, a CountVectorizer which can convert the raw text data into bag-of-words based count matrix, and implementations of both LDA and NMF. We'll of course also need the PLSA and EnsembleTopics classes from the enstop library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore') # Suppress deprecation warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohamed/.local/lib/python3.6/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "/home/mohamed/.local/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from enstop import EnsembleTopics, PLSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is getting the data. For this we can just use sklearn. First the ``fetch_20newsgroups`` function will download the 20-newsgroups data. By specifying ``subset='all'`` we collect the full dataset rather than either a train or test set. The next step in the process is to convert this text data into a form that can be consumed by LDA, NMF, PLSA and EnsembleTopics. The required format, in this case, is a matrix where the (i,j)th entry is the count of the number of times the jth word in the vocabulary occurs in the ith document (in this case each document is a newsgroup post). This can be done extremely efficiently using sklearn's ``CountVectorizer``. We'll pass two extra parameters to the ``CountVectorizer``: a setting of ``min_df=5`` which will restrict the vocabulary to words that occur at least 5 times in the entire corpus; and ``stop_words='english'`` which will eliminate common words (like \"the\", \"and\", etc.) accordingly to a dictionary of such words in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "news = fetch_20newsgroups(subset='all')\n",
    "data = CountVectorizer(min_df=5, stop_words='english').fit_transform(news.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to fit a model\n",
    "\n",
    "Now that we have the data is order, let's fit the various topic models and time them to see how long they take to fit. First up is LDA. The only parameter that requires tuning in this case is the number of topics we want to have. As a reasonable guess we'll choose 20 (the number of different newsgroups in the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lda_model = LatentDirichletAllocation(n_components=20).fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little over two minutes on my laptop -- not bad at all. It is worth noting the total CPU time, which came in at a little over four minutes, demonstrating that the implementation is making good use of parallelism, especially considering this is running on a two core processor.\n",
    "\n",
    "Next up is NMF. In this case we need a few extra parameters for the sklearn implementation. By default the sklearn NMF uses Frobenius loss -- essentially the total squared error between the data matrix and the reconstruction from the product two low rank matrices (with positive entries). While this is suitable for many uses it isn't the right loss for topic modeling. Instead we want to use the Kullback-Leibler loss, which essentially models the data as a set of independent Poisson's -- essentially it views the data as counts (which they are), and seeks the reconstruction from the product two low rank matrices to provide Poisson parameters that maximise the likelihood of seeing the data. Having changed the loss function we also need to change the solver from the classical coordinate descent to the multiplicative update based solver which can work with KL loss. All of this makes the NMF fitting process much slower, but it provides more accurate results for the purposes of topic modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nmf_model = NMF(n_components=20, beta_loss='kullback-leibler', solver='mu').fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around three and half minutes, so slower than LDA in this case. In general, especially for larger datasets than this, NMF will often tend to be as fast or sometimes even faster than LDA. In this case, however, it is a little slower. It's again worth noting the CPU time: over five minutes. Again, the implementation is making good use of parallelism on the two core processor.\n",
    "\n",
    "Next let's try EnsembleTopics. In this case we will specify ``n_components=20`` as with LDA and NMF, but this time that is more of a suggestion. EnsembleTopics will attempt to find a \"natural\" number of topics. Given that this is a small dataset we will also reduce the overall work to be done via the ``n_starts`` parameter, which specifies how many bootstrap runs of pLSA to try; for small data like this eight runs will likely suffice rather than the default 15. It is also beneficial to scale the parallelism a little -- since the processor only has two cores it is best not to overtax it with too many jobs at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ens_model = EnsembleTopics(n_components=20, n_starts=8, n_jobs=2).fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming in at over five minutes this is by far the slowest of the options we've tried here -- more than twice as long as LDA. On the other hand, as with NMF, EnsembleTopics will scale up well, and would likely look better on larger datasets. It is, however, in the right ballpark, which is somewhat reassuring. When we look at how well the topic modeling performed on the data this extra time might seem more worthwhile.\n",
    "\n",
    "Lastly let's look at pLSA. Historically pLSA is a precursor to LDA which came out a couple of years later and added Bayesian priors and more robust statistical foundations. On the other hand the pLSA algorithm itself is surprisingly simple, and with a little care high performance implementations are not hard to write. Given an efficient Expectation-Maximization optimizer it can potentially even find better solutions than a somewhat more complex LDA optimization. Using pLSA from enstop is just as easy as LDA in sklearn -- tell it the number of topics you want and set it going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plsa_model = PLSA(n_components=20).fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mere forty seconds! Decidedly faster than LDA, and it will scale well in terms of dataset size (but may scale less well in the desired number of topics). At the very least, given its speed, pLSA is a contender in the topic modeling space. Also worth noting is that this performance was achieved despite being a completely serial implementation -- the CPU time is the same as the wall time in this case.\n",
    "\n",
    "Now, having looked at how long it takes the algorithms to run, the next question is: how good are they? A fast algorithm that does a poor job is not worth much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality measures of topic models\n",
    "\n",
    "There are numerous ways to measure quality in topic models, including perplexity based approaches, lift, and coherence. Each technique has its pros and cons, as with any unsupervised task evaluation. We will attempt to sidestep some of these issues by evaluating the topic modeling approaches at a downstream task -- how well does the topic space categorise the different documents. Since the documents have defined labels (which newsgroup they were posted to) we have ground-truth to compare to. Since we can express the documents in terms of the learned topic space we can \"classify\" a document as the strongest topic associated to that document. Given two classifications we can then score how well these match via [adjusted Rand score](https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index) or [adjusted mutual information](https://en.wikipedia.org/wiki/Adjusted_mutual_information). Fortunately sklearn has implementations for both metrics. We'll also load numpy so we can extract the index of the most likely topic for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to place the documents in topic space. For NMF and LDA we can use the transform function. We could do the same for EnsembleTopics and pLSA, but since they store the document embedding in topic space of the training set as the ``embedding_`` attribute we can save work and just use that. Next we need to determine which topic is the most likely for each document -- this is just a matter of computing the argmax for each row of the embedded document matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_doc_vectors = nmf_model.transform(data)\n",
    "nmf_clusters = np.argmax(nmf_doc_vectors, axis=1)\n",
    "lda_doc_vectors = lda_model.transform(data)\n",
    "lda_clusters = np.argmax(lda_doc_vectors, axis=1)\n",
    "ens_doc_vectors = ens_model.embedding_\n",
    "ens_clusters = np.argmax(ens_doc_vectors, axis=1)\n",
    "plsa_doc_vectors = plsa_model.embedding_\n",
    "plsa_clusters = np.argmax(plsa_doc_vectors, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have classified the documents according to the topic space we can consider how well that classification compares to the actual ground-truth classification. Both adjusted Rand score and adjusted mutual information provide scores between 0 and 1 such that 0 represents an essentially random assignment (in comparison to the ground truth) and 1 represents a perfect matching with the ground truth. Obviosuly higher scores are better.\n",
    "\n",
    "We'll start with NMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NMF Adjusted Rand: \", adjusted_rand_score(news.target, nmf_clusters))\n",
    "print(\"NMF Adjusted Mutual Information: \", adjusted_mutual_info_score(news.target, nmf_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores are not great -- but the 20-newsgroups dataset is somewhat nontrivial (with several closely related newsgroups). Still, we can hope that some of the other techniques may have fared better.\n",
    "\n",
    "Next let's look at how the LDA model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LDA Adjusted Rand: \", adjusted_rand_score(news.target, lda_clusters))\n",
    "print(\"LDA Adjusted Mutual Information: \", adjusted_mutual_info_score(news.target, lda_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A definite improvement over NMF, but still below what we might ideally like. On the other hand LDA is considered the go-to state-of-the-art technique for topic modeling, so perhaps this is the best we can hope to do with this corpus and the (rather limited) amount of text-preprocessing we have done.\n",
    "\n",
    "Let's try pLSA next and see how it managed to do, given that it ran so very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pLSA Adjusted Rand: \", adjusted_rand_score(news.target, plsa_clusters))\n",
    "print(\"pLSA Adjusted Mutual Information: \", adjusted_mutual_info_score(news.target, plsa_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that pLSA outperformed LDA on this particular task, and by a reasonable amount. While the rand score is still fairly low the mutual information indicates that we are almost getting into a range that might be considered reasonable.\n",
    "\n",
    "Finally let's see what the extra work of ensembling can buy us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EnsTop Adjusted Rand: \", adjusted_rand_score(news.target, ens_clusters))\n",
    "print(\"EnsTop Adjusted Mutual Information: \", adjusted_mutual_info_score(news.target, ens_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that running several pLSA runs and looking for stable topics actually manages to produce much better topics, at least for classifying the 20-newsgroups posts. The even better news is that, despite getting the benefits of the pLSA approach, since the ensemble is built from bootstrap samples of the corpus we actually expect this to generalise better than the pure pLSA approach.\n",
    "\n",
    "We claim that, at least for this small example, EnsembleTopics is clearly the best approach for topic modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
